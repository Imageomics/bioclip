<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>BioCLIP: A Vision Foundation Model for the Tree of Life</title>
  <link rel="stylesheet" href="css/main.css" />
  <link rel="icon" href="images/icons/favicon.ico" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+Display:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel="stylesheet">
</head>

<body>
  <main>
    <div style="text-align: center">
      <details>
        <summary>More Research</summary>
        <div class="options">
          <p><a href="https://github.com/Imageomics/INTR">INTR</a></p>
          <p><a href="https://mmmu-benchmark.github.io">MMMU Benchmark</a></p>
          <p><a href="https://osu-nlp-group.github.io/Mind2Web">Mind2Web</a></p>
          <p><a href="https://osu-nlp-group.github.io/MagicBrush">MagicBrush</a></p>
        </div>
      </details>
    </div>
    <h1>BioCLIP: A Vision Foundation Model for the Tree of Life</h1>
    <p class="centered">
      <sup>1</sup><a href="https://samuelstevens.me/">Samuel Stevens</a><sup>*</sup>,
      <sup>1</sup>Jiaman (Lisa) Wu<sup>*</sup>,
      <sup>1</sup><a href="https://www.linkedin.com/in/thompson-m-j/">Matthew J Thompson</a>,
      <sup>1</sup><a href="https://egrace479.github.io/">Elizabeth G Campolongo</a>,
      <sup>1</sup><a href="https://chanh.ee/">Chan Hee (Luke) Song</a>,
      <sup>1</sup><a href="https://davidcarlyn.wordpress.com/">David Edward Carlyn</a>,
      <sup>2</sup><a href="https://dong.li/">Li Dong</a>,
      <sup>3</sup><a href="https://www.faculty.uci.edu/profile/?facultyId=6769">Wasila M Dahdul</a>,
      <sup>4</sup><a href="https://www.cs.rpi.edu/~stewart/">Charles Stewart</a>,
      <sup>1</sup><a href="https://cse.osu.edu/people/berger-wolf.1">Tanya Berger-Wolf</a>,
      <sup>1</sup><a href="https://sites.google.com/view/wei-lun-harry-chao">Wei-Lun (Harry) Chao</a>,
      <sup>1</sup><a href="https://ysu1989.github.io/">Yu Su</a>,
    </p>
    <p class="centered">
      <sup>1</sup>The Ohio State University,
      <sup>2</sup>Microsoft Research
      <sup>3</sup>University of California, Irvine
      <sup>4</sup>Rensselaer Polytechnic Institute
    </p>
    <p class="text-sm centered">
      <sup>*</sup>Sam and Lisa are co-first authors and contributed equally to BioCLIP.
    </p>
    <p class="text-sm centered">
      <a href="mailto:stevens.994@buckeyemail.osu.edu">stevens.994@osu.edu</a>, <a
        href="mailto:su.809@osu.edu">su.809@osu.edu</a>
    </p>

    <p class="centered">
      <a class="pill-button" href="https://huggingface.co/datasets/imageomics/TreeOfLife-10M">
        <img src="images/icons/huggingface.svg" /> Data
      </a>
      <a class="pill-button" href="https://huggingface.co/imageomics/bioclip">
        <img src="images/icons/huggingface.svg" /> Models
      </a>
      <a class="pill-button" href="https://github.com/Imageomics/BioCLIP">
        <img src="images/icons/github.svg" /> Code
      </a>
      <a class="pill-button" href="https://arxiv.org/abs/2311.18803">
        <img src="images/icons/arxiv.svg" /> Paper
      </a>
    </p>
    <figure>
      <img srcset="" src="images/hook.svg" alt="" loading="lazy">
      <figcaption>
        Figure 1: We use the CLIP objective (c) to train a ViT-B/16 on over 450K different class labels, all of which are taxonomic labels from the Tree of Life (a).
        Because the text encoder is an <i>autoregressive</i> language model, the <i>order</i> representation can only depend on higher ranks like <i>class</i>, <i>phlyum</i> and <i>kingdom</i> (b).
        This naturally leads to hierarchical representations for labels, helping the vision encoder learn image representations that are more aligned to the tree of life.
      </figcaption>
    </figure>

    <h2 class="banded">BioCLIP</h2>

    <p>
      Images of the natural world are a super-abundant source of biological information.
      There are many computational methods and tools, particularly computer vision, for extracting information from images.
      <b>But</b>, existing methods are bespoke models for a specific task and are not adaptable
      or extendable to new questions, contexts, and datasets.
    </p>
    <p>
      We develop the first large-scale multimodal model, <a href="https://huggingface.co/imageomics/bioclip">BioCLIP</a>, for general biology questions on images.
      We leverage the unique properties of biology as the the application domain for computer vision:
      <ol>
        <li>The abundance and variety of images about plants, animals, and fungi.</li>
        <li>The availability of rich structured biological knowledge.</li>
      </ol>
      We curate and release TreeOfLife-10M (the largest and most diverse available dataset of biology images), train BioCLIP, rigorously <a href="#evaluation">benchmark</a> our approach on diverse fine-grained biology classification tasks, and find that BioCLIP consistently and substantially outperforms existing baselines (by 17% to 20% absolute).
      <a href="#intrinsic">Intrinsic evaluation</a> further reveals that BioCLIP has learned a hierarchical representation conforming to the tree of life, shedding light on its strong generalizability.
    </p>
        
    <h3>Demo</h3>
    <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/4.7.1/gradio.js"></script>
    <gradio-app src="https://imageomics-bioclip-demo.hf.space"></gradio-app>

    <h2 class="banded" id="evaluation">Experiments</h2>
    <p>
      We evaluate BioCLIP and three baselines (<a href="">CLIP</a>, <a href="">OpenCLIP</a>, and an iNat-only model that
      uses the same procedure as BioCLIP but trained only on <a href="">iNat21</a>) on a diverse set of biology-related
      classification tasks.
      We do zero-shot classification with all models and report accuracy on the validation sets.
      Bold indicates the best performance for each task.
    </p>
    <p>
      <b>BioCLIP outperforms both general-domain baselines <i>and</i> our new iNat-only ViT model.</b>
    </p>
    <p>
      Check out the <a href="https://arxiv.org/abs/2311.18803">paper</a> for one-shot and five-shot results.
    </p>
    <p><i>Scroll to see all results.</i></p>
    <table cellpadding="0" cellspacing="0">
      <thead>
        <tr>
          <th rowspan="2" class="sticky border-right">Model</th>
          <th colspan="4" class="no-border-bottom border-right">Animals</th>
          <th colspan="5" class="no-border-bottom border-right">Plants & Fungi</th>
          <th rowspan="2" class="border-right">Rare Species</th>
          <th rowspan="2">Mean</th>
        </tr>
        <tr>
          <th class="no-border-top border-left">Birds 525</th>
          <th class="no-border-top">Plankton</th>
          <th class="no-border-top">Insects</th>
          <th class="no-border-top border-right">Insects 2</th>
          <th class="no-border-top">PlantNet</th>
          <th class="no-border-top">Fungi</th>
          <th class="no-border-top">PlantVillage</th>
          <th class="no-border-top">Med. Leaf</th>
          <th class="no-border-top border-right">PlantDoc</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td class="sticky border-right">CLIP</td>
          <td>49.9</td>
          <td>3.2</td>
          <td>9.1</td>
          <td>9.8</td>
          <td>58.5</td>
          <td>10.2</td>
          <td>5.4</td>
          <td>15.9</td>
          <td>26.1</td>
          <td>26.6</td>
          <td>21.4</td>
        </tr>
        <tr>
          <td class="sticky border-right">OpenCLIP</td>
          <td>54.7</td>
          <td>2.2</td>
          <td>6.5</td>
          <td>9.6</td>
          <td>50.2</td>
          <td>5.7</td>
          <td>8.0</td>
          <td>12.4</td>
          <td>25.8</td>
          <td>31.0</td>
          <td>20.6</td>
        </tr>
        <tr>
          <td class="sticky border-right">BioCLIP</td>
          <td><b>72.1</b></td>
          <td><b>6.1</b></td>
          <td><b>34.8</b></td>
          <td><b>20.4</b></td>
          <td><b>91.4</b></td>
          <td>40.7</td>
          <td><b>24.4</b></td>
          <td><b>38.6</b></td>
          <td><b>28.4</b></td>
          <td><b>37.8</b></td>
          <td><b>39.4</b></td>
        </tr>
        <tr>
          <td class="sticky border-right">iNat21 Only</td>
          <td>56.1</td>
          <td>2.6</td>
          <td>30.7</td>
          <td>11.5</td>
          <td>88.2</td>
          <td><b>43.0</b></td>
          <td>18.4</td>
          <td>25.6</td>
          <td>20.5</td>
          <td>19.4</td>
          <td>31.6</td>
        </tr>
      </tbody>
    </table>

    <h3>Evaluation Examples</h3>
    <div class="figure-container">
      <figure>
        <a href="images/bioclip_correct_fungi.png">
          <img srcset="" src="images/bioclip_correct_fungi.png" alt="" loading="lazy">
        </a>
        <figcaption>
          BioCLIP correctly labels this <a href="https://en.wikipedia.org/wiki/Clitocybe_fragrans"><i>Clitocybe fragrans</i></a> from the Fungi task.
        </figcaption>
      </figure>
      <figure>
        <a href="images/bioclip_correct_med-leaf.png">
          <img srcset="" src="images/bioclip_correct_med-leaf.png" alt="" loading="lazy">
        </a>
        <figcaption>
          BioCLIP correctly labels this Jasmine leaf (<a href="https://en.wikipedia.org/wiki/Jasmine"><i>Jasminum</i></a>) from the Medicinal Leaf task.
        </figcaption>
      </figure>
      <figure>
        <a href="images/bioclip_correct_plankton.png">
          <img srcset="" src="images/bioclip_correct_plankton.png" alt="" loading="lazy">
        </a>
        <figcaption>
          BioCLIP correctly labels this <a href="https://microbewiki.kenyon.edu/index.php/Rhizosolenia"><i>Rhizosolenia</i></a> from the Plankton task.
          BioCLIP performs well on a variety of image sources, like this microscope image.
        </figcaption>
      </figure>
      <figure>
        <a href="images/openai_wrong_bioclip_right_openai_fungi.png">
          <img srcset="" src="images/openai_wrong_bioclip_right_openai_fungi.png" alt="" loading="lazy">
        </a>
        <figcaption>
          CLIP mislabels this <a href="https://en.wikipedia.org/wiki/Russula_ochroleuca"><i>Russula ochroleuca</i></a> as "the prince" (<a href="https://en.wikipedia.org/wiki/Agaricus_augustus"><i>Agaricus augustus</i></a>), which isn't even in the same family as Russula ochroleuca, while BioCLIP correctly labels it.
        </figcaption>
      </figure>
      <figure>
        <a href="images/openai_wrong_bioclip_right_openai_med-leaf.png">
          <img srcset="" src="images/openai_wrong_bioclip_right_openai_med-leaf.png" alt="" loading="lazy">
        </a>
        <figcaption>
          CLIP mislabels this Roxburg fig (<a href="https://en.wikipedia.org/wiki/Ficus_auriculata"><i>Ficus auriculata</i></a>) as a Peepal tree (<a href="https://en.wikipedia.org/wiki/Ficus_religiosa"><i>Ficus Religiosa</i></a>), while BioCLIP correctly labels it.
        </figcaption>
      </figure>
      <figure>
        <a href="images/openai_wrong_bioclip_right_openai_plankton.png">
          <img srcset="" src="images/openai_wrong_bioclip_right_openai_plankton.png" alt="" loading="lazy">
        </a>
        <figcaption>
          CLIP mislabels this <a href="https://eol.org/pages/898634"><i>Strombidium concicum</i></a> as a ciliate mix, while BioCLIP correctly labels it.
        </figcaption>
      </figure>
    </div>

    <h2 id="intrinsic" class="banded">Intrinsic Evaluation</h2>
    <p>
      Why does BioCLIP work so well?
      We conduct an intrinsic evaluation to understand the representations learned by BioCLIP.
      We visualize BioCLIP and CLIP's representations for the 100K unseen images in the iNat21 validation set, using T-SNE to plot them in two-dimensions, coloring the points based on their class.
      In the figure below, (B) means <b>B</b>ioCLIP and (O) means <b>O</b>penAI's CLIP.
    </p>
    <p>
      At higher ranks like phylum, both CLIP and BioCLIP have good separation, but you can see that BioCLIP's representation is more fine-grained and contains a richer clustering structure.
      At lower ranks, BioCLIP produces far more separable features, while CLIP's features tend to be cluttered and lack a clear structure.
      This shows that BioCLIP has learned a rich feature representation following the hierarchical structure of the taxonomy, which helps explain its strong generalization across the tree of life.
    </p>
    <figure>
      <a href="files/intrinsic.pdf">
        <img srcset="" src="images/intrinsic_small.png" alt="" loading="lazy">
      </a>
      <figcaption>
        We show that <b>(B)</b>ioCLIP's representations are more fine-grained and contain a richer clustering structure than <b>(O)</b>penAI's CLIP.
        Click on the image to see the full resolution PDF, or check out our <a href="https://arxiv.org/abs/2311.18803">paper</a> for more details.
      </figcaption>
    </figure>

    <h2 class="banded" id="dataset">Dataset</h2>
    <p>
      TreeOfLife-10M is the largest and most diverse available dataset of biology images.
      We combine images from three sources, iNaturalist, BIOSCAN-1M, and Encyclopedia of Life (<a href="https://eol.org">EOL</a>, accessed 29 July 2023), to create a dataset of
      10M images, spanning 450K+ species.
      <!-- We provide a train/val split, and will release the dataset under a CC-BY-NC license. -->
      We train BioCLIP on TreeOfLife-10M and release the weights for public use.
    </p>

    <h2>Reference</h2>
    <p>Please cite our paper if you use our code, data, model or results.</p>
    <pre class="reference">@article{stevens2023bioclip,
  title={BioCLIP: A Vision Foundation Model for the Tree of Life}, 
  author={Samuel Stevens and Jiaman Wu and Matthew J Thompson and Elizabeth G Campolongo and Chan Hee Song and David Edward Carlyn and Li Dong and Wasila M Dahdul and Charles Stewart and Tanya Berger-Wolf and Wei-Lun Chao and Yu Su},
  year={2023},
  eprint={2311.18803},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
}</pre>
    <p>Also consider citing OpenCLIP, iNat21 and BIOSCAN-1M:</p>
    <pre class="reference">@software{ilharco_gabriel_2021_5143773,
  author={Ilharco, Gabriel and Wortsman, Mitchell and Wightman, Ross and Gordon, Cade and Carlini, Nicholas and Taori, Rohan and Dave, Achal and Shankar, Vaishaal and Namkoong, Hongseok and Miller, John and Hajishirzi, Hannaneh and Farhadi, Ali and Schmidt, Ludwig},
  title={OpenCLIP},
  year={2021},
  doi={10.5281/zenodo.5143773},
}</pre>
    <pre class="reference">@misc{inat2021,
  author={Van Horn, Grant and Mac Aodha, Oisin},
  title={iNat Challenge 2021 - FGVC8},
  publisher={Kaggle},
  year={2021},
  url={https://kaggle.com/competitions/inaturalist-2021}
}</pre>
    <pre class="reference">@inproceedings{gharaee2023step,
  author={Gharaee, Z. and Gong, Z. and Pellegrino, N. and Zarubiieva, I. and Haurum, J. B. and Lowe, S. C. and McKeown, J. T. A. and Ho, C. Y. and McLeod, J. and Wei, Y. C. and Agda, J. and Ratnasingham, S. and Steinke, D. and Chang, A. X. and Taylor, G. W. and Fieguth, P.},
  title={A Step Towards Worldwide Biodiversity Assessment: The {BIOSCAN-1M} Insect Dataset},
  booktitle={Advances in Neural Information Processing Systems ({NeurIPS}) Datasets \& Benchmarks Track},
  year={2023},
}</pre>
  </main>
  <script>
    const details = document.querySelector("details");
    document.addEventListener("click", function (e) {
      if (!details.contains(e.target)) {
        details.removeAttribute("open");
      }
    });
  </script>
</body>

</html>
